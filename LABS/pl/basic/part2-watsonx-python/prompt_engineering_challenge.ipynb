{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zaawansowany Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Witaj w drugiej części laboratorium poświęconemu prompt engineeringowi. Tym razem będziemy odpytywać watsonx.ai z poziomu kodu przygotowanego w Pythonie. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Załaduj niezbędne biblioteki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#from dotenv import load_dotenv\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "import json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Załaduj swoje credentiale do środowiska Watsonx.ai\n",
    "    - Aby uzyskać api_key, przejdź do strony głównej folderu laboratorium, na której znajdziesz instrukcję,\n",
    "    - Aby uzyskać projekt_id, przejdź do strony głównej folderu laboratorium, na której znajdziesz instrukcję,\n",
    "    - Jako endpoint, użyj \"https://us-south.ml.cloud.ibm.com\".\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name\n",
    "#google/flan-ul2\n",
    "#google/flan-t5-xxl\n",
    "#ibm/mpt-7b-instruct2\n",
    "#EleutherAI/gpt-neox-20b\n",
    "\n",
    "#decoding_method\n",
    "#greedy\n",
    "#sampling\n",
    "\n",
    "#Skonfiguruj środowisko watsonx.ai\n",
    "my_credentials = {\n",
    "    \"url\" : \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"apikey\" : \"\"\n",
    "}\n",
    "project_id = \"\"\n",
    "\n",
    "def send_to_watsonxai(prompt,\n",
    "                    model_id=ModelTypes.FLAN_UL2,\n",
    "                    decoding_method=\"greedy\",\n",
    "                    max_new_tokens=100, #maximum value - 500\n",
    "                    min_new_tokens=30, #minimum value - 0\n",
    "                    temperature=1.0,\n",
    "                    repetition_penalty=2.0\n",
    "                    ):\n",
    "    '''\n",
    "   helper function for sending prompts and params to Watsonx.ai\n",
    "    \n",
    "    Args:  \n",
    "        prompts:list list of text prompts\n",
    "        decoding:str Watsonx.ai parameter \"sample\" or \"greedy\"\n",
    "        max_new_tok:int Watsonx.ai parameter for max new tokens/response returned\n",
    "        temp:float Watsonx.ai parameter for temperature (range 0>2)\n",
    "\n",
    "    Returns: None\n",
    "        prints response\n",
    "    '''\n",
    "\n",
    "    space_id = None\n",
    "    verify = False\n",
    "    \n",
    "    generate_params = {\n",
    "        GenParams.DECODING_METHOD: decoding_method,\n",
    "        GenParams.MAX_NEW_TOKENS: max_new_tokens,\n",
    "        GenParams.MIN_NEW_TOKENS: min_new_tokens,\n",
    "        GenParams.TEMPERATURE: temperature,\n",
    "        GenParams.REPETITION_PENALTY: repetition_penalty,\n",
    "        GenParams.RANDOM_SEED: 42\n",
    "    }\n",
    "\n",
    "    # Instantiate a model proxy object to send your requests\n",
    "    model = Model( model_id, my_credentials, generate_params, project_id, space_id, verify )   \n",
    "\n",
    "    result = model.generate(prompt)\n",
    "    print(result[\"results\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zadanie 1) Poziom a1c pacjenta określa jego status cukrzycowy, zasady są następujące:\n",
    "\n",
    " - mniej niż 5.7 oznacza brak cukrzycy\n",
    " - pomiędzy 5.7 a 6.5 stan przedcukrzycowy\n",
    " - więcej niż 6.5 oznacza stan cukrzycowy.\n",
    "\n",
    "Na podstawie poniższych trzech przykładów, napisz prompt który zwróci status cukrzycowy opisanych przypadków:\n",
    "\n",
    "1)\tThe patients a1c is 5.5 which is good considering his other risk factors.\n",
    "2)\tFrom the last lab report I noted the A1c is 6.4 so we need to put her on Ozempic.\n",
    "3)\tShe mentioned her A1c is 8 according to her blood work about 3 years ago.\n",
    "\n",
    "Bonus 1: Jak możesz poprawić wnioskowanie, biorąc pod uwagę inne informacje w zdaniach?\n",
    "\n",
    "Bonus 2: jak podszedłbyś do wyodrębnienia statusu cukrzycy na podstawie notatek pacjenta bez wartości A1C i na co musiałbyś uważać? (wskazówka: może mówią o historii choroby w rodzinie lub innych komplikacjach)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello ', 'generated_token_count': 100, 'input_token_count': 4, 'stop_reason': 'MAX_TOKENS'}]\n"
     ]
    }
   ],
   "source": [
    "#Q1 ENTER YOUR MODEL PARAMS HERE - MAKE SURE IT WORKS WITH ALL 3 EXAMPLES ABOVE\n",
    "\n",
    "\n",
    "prompt = \"\"\n",
    "\n",
    "\n",
    "\n",
    "response = send_to_watsonxai(prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opinia produktu dla zadań 2-6\n",
    "prompt = \"\"\"Needed a nice lamp for my bedroom, and this one had \\\n",
    "additional storage and not too high of a price point. \\\n",
    "Got it fast.  The string to our lamp broke during the \\\n",
    "transit and the company happily sent over a new one. \\\n",
    "Came within a few days as well. It was easy to put \\\n",
    "together.  I had a missing part, so I contacted their \\\n",
    "support and they very quickly got me the missing piece! \\\n",
    "Lumina seems to me to be a great company that cares \\\n",
    "about their customers and products!!\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zadanie 2) Napisz prompt, który zwróci sentyment związany z podaną opinią\n",
    "Target sentiment = positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "#Q2 Code - enter prompt and parameters in this cell\n",
    "prompt = \"\"\n",
    "response = send_to_watsonxai(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
